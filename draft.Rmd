---
title: "datasym"
author: "Erika Vargas"
date: "May 27, 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load data sorted by year and month to reflect the first sale of each producer/agent
library(aod)
library(ggplot2)
library(dplyr)
library(readxl)
DATAORG <- read_excel("~/Desktop/SPRING 2019/CAPSTONE/DATAORG.xlsx")
head(DATAORG)
DATAORG<- DATAORG[,-c(13,14,15)]
```

#Cleaning Data Process
```{r}
sapply(DATAORG,function(x) sum(is.na(x)))   #checking for missing values*
sapply(DATAORG, function(x) length(unique(x)))  #unique values there are for each variable
```


```{r}
# select distinct producers (eliminating duplicates)
DATASYM <- subset(DATAORG, !duplicated(ContactID))
nrow(DATASYM)  #number of unique agents(contactID)
```

```{r}
#deleting rows with NA values 
DATASYM <- na.omit(DATASYM)
nrow(DATASYM)
sapply(DATASYM,function(x) sum(is.na(x))) 
#changing character columns to factors 
DATASYM <- mutate_at(DATASYM, vars(Channel,Specialist,`onlySale?`,Parent,FirmName,State,PSuiteCT, Product,Year, Month), as.factor)
str(DATASYM)
summary(DATASYM)
```

# Full model 
```{r}
modelfull <- glm(`onlySale?` ~ Channel + Specialist + State + PSuiteCT + Year + Month + SaleAmount,family=binomial(link='logit'),data=DATASYM)
summary(modelfull)
anova(modelfull, test="Chisq")
library(pscl)
pR2(modelfull)
plot(modelfull)

```

**After evaluating the full model, I decided to split the dataset into two chunks: training and testing set. The training set will be used to fit the full model which we will be testing over the testing set.**

# Model Fitting
```{r}
#splitting the data into train and test datasets 
require(caTools)  # loading caTools library
set.seed(123)   #  set seed to ensure you always have same random numbers generated

sample = sample.split(DATASYM,SplitRatio = 0.75) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
train =subset(DATASYM,sample ==TRUE) # creates a training dataset named train with rows which are marked as TRUE
test=subset(DATASYM, sample==FALSE)
```


```{r}
model1 <- glm(`onlySale?` ~ Channel + Specialist + State + PSuiteCT + Year + Month + SaleAmount,family=binomial(link='logit'),data=train)
summary(model1)
```

*Interpreting the results of our logistic regression model*
Now I can analyze the fitting and interpret what the model is telling us.
First of all, we can see that State, and some of the Specialists are not statistically significant. As for the statistically significant variables, Channel, PSuite, Month, year, and sale amount had the lowest p-value suggesting a strong association of these variables respect to agents with the probability of having only one sale. The Positive coefficient for this Channel predictor suggests that all other variables being equal, that agents selling through independent channel are more likely to sell once. In this logit model the response variable is log odds: ln(odds) = ln(p/(1-p)) = a*x1 + b*x2 + â€¦ + z*xn. Since channel is a dummy variable, independent reduces the log odds by 0.8 while a unit increase in Sales amount reduces the log odds by 0.000000465.


```{r}
anova(model1, test="Chisq")
```

The difference between the null deviance and the residual deviance shows how the model is doing against the null model (a model with only the intercept). The wider this gap, the better. Analyzing the table we can see the drop in deviance when adding each variable one at a time. Again, adding Channel, Specialist, and PSuiteCT, and Month significantly reduces the residual deviance. The other variables seem to improve the model less even though Sale amount has a low p-value. A large p-value here (state) indicates that the model without the variable explains more or less the same amount of variation. 

```{r}
library(pscl)
pR2(model1)
```
While no exact equivalent to the R2 of linear regression exists, the McFadden R2 index can be used to assess the model fit. in this case this model explains 16.89% of variation.


#Assessing the predictive ability of the model

```{r}
fitted.results <- predict(model1,newdata=subset(test,select=c(1,2,4,7,8,10,11,12)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$`onlySale?`)
print(paste('Accuracy',1-misClasificError))
```



# ROC Curve

I am going to plot the ROC curve and calculate the AUC (area under the curve) which are typical performance measurements for a binary classifier.
The ROC is a curve generated by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings while the AUC is the area under the ROC curve. As a rule of thumb, a model with good predictive ability should have an AUC closer to 1 (1 is ideal) than to 0.5.

```{r}
library(ROCR)
p <- predict(model1, newdata=subset(test,select=c(1,2,4,7,8,10,11,12)), type="response")
pr <- prediction(p, test$`onlySale?`)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```

this model has a 0.7769 AUC which means that the model has a fair predictive ability 



#Second Model taking State variable off
```{r}
model2 <- glm(`onlySale?` ~ Channel + Specialist + PSuiteCT + Year + Month + SaleAmount,family=binomial(link='logit'),data=train)
summary(model2)
```

```{r}
anova(model2, test="Chisq")
```

```{r}
pR2(model2)
```

```{r}
fitted.results2 <- predict(model2,newdata=subset(test,select=c(1,2,4,8,10,11,12)),type='response')
fitted.results2 <- ifelse(fitted.results2 > 0.5,1,0)
misClasificError2 <- mean(fitted.results2 != test$`onlySale?`)
print(paste('Accuracy',1-misClasificError2))
```


```{r}
library(ROCR)
p2 <- predict(model2, newdata=subset(test,select=c(1,2,4,8,10,11,12)), type="response")
pr2 <- prediction(p2, test$`onlySale?`)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
plot(prf2)

auc2 <- performance(pr2, measure = "auc")
auc2 <- auc2@y.values[[1]]
auc2
```


```{r}
library(stargazer)
stargazer(modelfull,model1,model2, TITLE="Logistic Regression Results" , kee.stat = c("n", "rsq"), align = TRUE, font.size = "small", type = "text") 
```


